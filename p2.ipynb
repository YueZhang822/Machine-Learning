{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FD0LkiPhSH1Z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yzh/miniconda3/envs/grace-test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch, math, copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0KZgSc2SH1o"
      },
      "source": [
        "# From Shallow to Deep Neural Networks\n",
        "\n",
        "The main goal of this assignment is to develop a better understanding of how the depth of a network interacts with its trainability and performance.\n",
        "\n",
        "In the previous assignment you likely observed difficulties in training sigmoid and ReLU networks with over ~8 layers, which is typically associated with 'vanishing' or 'exploding' gradients. As you will see, some of the biggest achievements in deep learning have been the development of techniques that enable deeper networks to be successfully trained, and without them deep networks are notoriously difficult to train successfully.\n",
        "\n",
        "You will be working with the MNIST dataset, which will be downloaded and loaded in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b_qmffSoSH1o"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "train_dataset = datasets.MNIST(\"data\", train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(\"data\", train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=256, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIviUo83SH1o"
      },
      "source": [
        "Fill the missing code below. In both train_epoch and test, total_correct should be the total number of correctly classified samples, while total_samples should be the total number of samples that have been iterated over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MIHugdT9SH1o"
      },
      "outputs": [],
      "source": [
        "def train(epochs, model, criterion, optimizer, train_loader, test_loader):\n",
        "    for epoch in range(epochs):\n",
        "        train_err = train_epoch(model, criterion, optimizer, train_loader)\n",
        "        test_err = test(model, test_loader)\n",
        "        print('Epoch {:03d}/{:03d}, Train Error {:.2f}% || Test Error {:.2f}%'.format(epoch, epochs, train_err*100, test_err*100))\n",
        "    return train_err, test_err\n",
        "\n",
        "def train_epoch(model, criterion, optimizer, loader):\n",
        "    total_correct = 0.\n",
        "    total_samples = 0.\n",
        "\n",
        "    #model.train()\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        # insert code to feed the data to the model and collect its output\n",
        "        output = model(data)\n",
        "\n",
        "        # insert code to compute the loss from output and the true target\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # insert code to update total_correct and total_samples\n",
        "        # total_correct: total number of correctly classified samples\n",
        "        # total_samples: total number of samples seen so far\n",
        "        total_correct += (output.argmax(dim=1) == target).sum().item()\n",
        "        total_samples += target.size(0)\n",
        "\n",
        "        # insert code to update the parameters using optimizer\n",
        "        # be careful in this part as an incorrect implementation will affect\n",
        "        # all your experiments and have a significant impact on your grade!\n",
        "        # in particular, note that pytorch does --not-- automatically\n",
        "        # clear the parameter's gradients: check tutorials to see\n",
        "        # how this can be done with a single method call.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return 1 - total_correct/total_samples\n",
        "\n",
        "def test(model, loader):\n",
        "    total_correct = 0.\n",
        "    total_samples = 0.\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # insert code to feed the data to the model and collect its output\n",
        "            output = model(data)\n",
        "\n",
        "            # insert code to update total_correct and total_samples\n",
        "            # total_correct: total number of correctly classified samples\n",
        "            # total_samples: total number of samples seen so far\n",
        "            total_correct += (output.argmax(dim=1) == target).sum().item()\n",
        "            total_samples += target.size(0)\n",
        "\n",
        "    return 1 - total_correct/total_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cTYkX0GSH1p"
      },
      "source": [
        "### CNN with Tanh activations\n",
        "\n",
        "Next, you should implement a baseline model so you can check how increasing the number of layers can make a network considerably harder to train, given that no additional methods such as residual connections and normalization layers are adopted.\n",
        "\n",
        "Finish the implementation of CNNtanh below, carefully following the specifications:\n",
        "\n",
        "The model should have exactly 'k' many convolutional layers, followed by a linear (fully-connected) layer that actually outputs the logits for each of the 10 MNIST classes.\n",
        "\n",
        "The network should consist of 3 stages, each with k/3 many convolutional layers (you can assume k is divisible by 3). Each conv layer should have a 3x3 kernel, a stride of 1 and a padding of 1 pixel (such that the output of the convolution has the same height and width as its input).\n",
        "\n",
        "It should also have an average pooling layer at the end of each stage, with a 2x2 window (hence halving the spatial dimensions), and the number of channels should double from one stage to the other (starting with 4 in the first stage). Moreover, a Tanh activation should follow each convolution layer.\n",
        "\n",
        "When k=3, for example, the network should be:\n",
        "\n",
        "1. Stage 1 (1x28x28 input, 4x14x14 output):\n",
        "    1. Conv layer with 1 input channel and 4 output channels, 3x3 kernel, stride=padding=1\n",
        "    2. Tanh activation\n",
        "    3. Average Pool with 2x2 kernel and stride 2\n",
        "2. Stage 2 (4x14x14 input, 8x7x7 output):\n",
        "    1. Conv layer with 4 input channels and 8 output channels, 3x3 kernel, stride=padding=1\n",
        "    2. Tanh activation\n",
        "    3. Average Pool with 2x2 kernel and stride 2\n",
        "3. Stage 3 (8x7x7 input, 16x3x3 output):\n",
        "    1. Conv layer with 8 input channels and 16 output channels, 3x3 kernel, stride=padding=1\n",
        "    2. Tanh activation\n",
        "    3. Average Pool with 2x2 kernel and stride 2\n",
        "4. Fully-connected layer with 16 * 3 * 3=144 input dimension and 10 output dimension\n",
        "\n",
        "Note that the model should not have any activation after the fully-connected layer: the PyTorch loss module that will be adopted takes logits as input and not class probabilities.\n",
        "\n",
        "In contrast to the network exemplified above with k=3, when k=6 it should have two conv layers per stage instead of one (each one with a tanh activation following it).\n",
        "\n",
        "Lastly, do not change the code block with a for loop in the end of init: its purpose to randomly initialize the parameters of the conv layers by sampling from a Gaussian with zero mean and 0.05 deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xc9pl4t_SH1p"
      },
      "outputs": [],
      "source": [
        "class CNNtanh(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(CNNtanh, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                layers_list.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1))\n",
        "                layers_list.append(nn.Tanh())\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc = nn.Linear(channel_in*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                m.weight.data.normal_(0, 0.05)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        for stage in self.conv_stages:\n",
        "            x = stage(x)\n",
        "        u = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "        return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx_4tOu4uiFW"
      },
      "source": [
        "The line below just instantiates the PyTorch Cross Entropy loss, whose inputs should be logits: hence the reason that the CNN should not have an activation after last (feedforward) layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P4vxR4FOSH1p"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv54ar5_SH1p"
      },
      "source": [
        "Now, you should train CNNtanh with different values for k: your goal is to find the largest value for k such that the network achieves less than 20% error (either train or test) in 3 epochs. You should also choose an appropriate learning rate (but do not change the optimizer or the momentum settings!).\n",
        "\n",
        "Note that CNNs can easily achieve under 2% test error on MNIST, but we're choosing 20% as a threshold since you will be training each network for only 3 epochs.\n",
        "\n",
        "Remember to use values for k that are divisible by 3. When submitted, your notebook should have the training log of a network with two consecutive values for k (for example, 6 and 9) such that the network is 'trainable' with the smaller one but not 'trainable' with the larger one. It is fine for the training log to include runs with more than two values of k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training Tanh CNN with 6 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 73.45% || Test Error 21.00%\n",
            "Epoch 001/003, Train Error 11.39% || Test Error 6.36%\n",
            "Epoch 002/003, Train Error 5.18% || Test Error 3.62%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 6\n",
        "print(\"\\033[33m\\nTraining Tanh CNN with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNtanh(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training Tanh CNN with 9 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 88.80% || Test Error 88.65%\n",
            "Epoch 001/003, Train Error 88.76% || Test Error 88.65%\n",
            "Epoch 002/003, Train Error 88.85% || Test Error 88.65%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 9\n",
        "print(\"\\033[33m\\nTraining Tanh CNN with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNtanh(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k8-1IeISH1q"
      },
      "source": [
        "### Better Initialization\n",
        "\n",
        "Next, we will change the initialization of the conv layers and see how it affects the trainability of deep networks. Instead of sampling from a Gaussian with a deviation of 0.05, you should sample from a Gaussian with a deviation $\\sigma = \\sqrt{\\frac{1}{k^2 \\cdot C_{out}}}$ or $\\sigma = \\sqrt{\\frac{1}{k^2 \\cdot C_{in}}}$, where $k$ is the kernel size ($k=3$ for 3x3 convolutions), $C_{in}$ is the number of input channels, and $C_{out}$ the number of output channels.\n",
        "\n",
        "The model below should be exactly like CNNtanh except for the standard deviation of the normal distribution used to initialize the conv layers.\n",
        "\n",
        "The paper 'Understanding the difficulty of training deep feedforward neural networks' by Glorot and Bengio provides some intuition behind such a choice for $\\sigma$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mLCUVVAJfm25"
      },
      "outputs": [],
      "source": [
        "class CNNtanh_newinit(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(CNNtanh_newinit, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                layers_list.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1))\n",
        "                layers_list.append(nn.Tanh())\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc = nn.Linear(channel_in*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1 / (m.kernel_size[0] * m.kernel_size[1] * m.out_channels))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        for stage in self.conv_stages:\n",
        "            x = stage(x)\n",
        "        u = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "        return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_1LxL-Y0Q2A"
      },
      "source": [
        "Repeat the procedure of finding the maximum number of layers such that the network is still trainable, this time with CNNtanhinit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training Tanh CNN + new init with 48 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 52.93% || Test Error 12.13%\n",
            "Epoch 001/003, Train Error 7.00% || Test Error 5.01%\n",
            "Epoch 002/003, Train Error 4.31% || Test Error 3.48%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 48\n",
        "print(\"\\033[33m\\nTraining Tanh CNN + new init with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNtanh_newinit(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training Tanh CNN + new init with 51 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 89.22% || Test Error 88.65%\n",
            "Epoch 001/003, Train Error 89.06% || Test Error 88.65%\n",
            "Epoch 002/003, Train Error 88.94% || Test Error 88.65%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 51\n",
        "print(\"\\033[33m\\nTraining Tanh CNN + new init with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNtanh_newinit(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkVRwRkW0rkQ"
      },
      "source": [
        "### CNN with ELU activations\n",
        "\n",
        "In this section you should replace the Tanh activations of the previous network for Exponential Linear Units (ELUs). Complete CNNelu below, which should be exactly like CNNtanhinit except for ELU activations instead of Tanh (ELUs are readily available in PyTorch, check its documentation for more details)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xFyLrax5SH1t"
      },
      "outputs": [],
      "source": [
        "class CNNelu(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(CNNelu, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                layers_list.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1))\n",
        "                layers_list.append(nn.ELU())\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc = nn.Linear(channel_in*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1 / (m.kernel_size[0] * m.kernel_size[1] * m.out_channels))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        for stage in self.conv_stages:\n",
        "            x = stage(x)\n",
        "        u = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "        return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2toBBTt1ZU5"
      },
      "source": [
        "Repeat the procedure of finding the maximum number of layers such that the network is still trainable, this time with CNNelu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ELU CNN, with 39 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 41.42% || Test Error 6.39%\n",
            "Epoch 001/003, Train Error 4.58% || Test Error 3.49%\n",
            "Epoch 002/003, Train Error 3.09% || Test Error 2.31%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 39\n",
        "print(\"\\033[33m\\nTraining ELU CNN, with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNelu(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ELU CNN, with 42 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 88.56% || Test Error 90.20%\n",
            "Epoch 001/003, Train Error 90.13% || Test Error 90.20%\n",
            "Epoch 002/003, Train Error 90.13% || Test Error 90.20%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 42\n",
        "print(\"\\033[33m\\nTraining ELU CNN, with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNelu(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjhF18tS1hVw"
      },
      "source": [
        "### CNN with Batch Normalization\n",
        "\n",
        "Next, you will check how batch normalization can make deep networks easier to train. Implement the network below, which should be exactly like CNNelu except for additional BatchNorm2d layers after each convolution (before the ELU activation).\n",
        "\n",
        "Note that BatchNorm2d modules require the number of channels as argument -- see the PyTorch documentation for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AHqz0bqbDKEo"
      },
      "outputs": [],
      "source": [
        "class CNNeluBN(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(CNNeluBN, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                layers_list.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1))\n",
        "                layers_list.append(nn.BatchNorm2d(channel_out))\n",
        "                layers_list.append(nn.ELU())\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc = nn.Linear(channel_in*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1 / (m.kernel_size[0] * m.kernel_size[1] * m.out_channels))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        for stage in self.conv_stages:\n",
        "            x = stage(x)\n",
        "        u = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "        return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WqDqDmG2eAo"
      },
      "source": [
        "Repeat the procedure of finding the maximum number of layers such that the network is still trainable, this time with CNNeluBN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ELU CNN + BN with 63 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 14.75% || Test Error 4.09%\n",
            "Epoch 001/003, Train Error 4.10% || Test Error 3.69%\n",
            "Epoch 002/003, Train Error 2.84% || Test Error 1.95%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 63\n",
        "print(\"\\033[33m\\nTraining ELU CNN + BN with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNeluBN(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ELU CNN + BN with 66 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 25.83% || Test Error 8.61%\n",
            "Epoch 001/003, Train Error 89.09% || Test Error 90.20%\n",
            "Epoch 002/003, Train Error 90.13% || Test Error 90.20%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 66\n",
        "print(\"\\033[33m\\nTraining ELU CNN + BN with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNeluBN(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LOZfZtl2gcg"
      },
      "source": [
        "### Residual Networks\n",
        "\n",
        "Finally, you experiment adding residual connections to a CNN.\n",
        "\n",
        "To implement the model below, you should add a 'skip connection' to 'Conv->BatchNorm->ELU' blocks whenever the shape of the block's input and output are the same: this will be the case for every such block except for the first ones in each stage, as they double the number of channels.\n",
        "\n",
        "More specifically, you should change $u = ELU(BatchNorm(Conv(x)))$ to $u = ELU(BatchNorm(Conv(x))) + x$, where $x$ and $u$ denote the block's input and output, respectively.\n",
        "\n",
        "You should take your CNNeluBN implementation and add skip-connections as described above.\n",
        "\n",
        "Note that there are key differences between the resulting model and the actual ResNet proposed by He et al. in 'Deep Residual Learning for Image Recognition', for example the use of ELU activations instead of ReLU and the exact position of skip-connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "wA6ItpH-GpK_"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(channel_out),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x) + x\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                if channel_in == channel_out:\n",
        "                    layers_list.append(self.residual_block(channel_in, channel_out))\n",
        "                else:\n",
        "                    layers_list.append(self.basic_block(channel_in, channel_out))\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc = nn.Linear(channel_in*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1 / (m.kernel_size[0] * m.kernel_size[1] * m.out_channels))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def basic_block(self, channel_in, channel_out):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(channel_out),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "    def residual_block(self, channel_in, channel_out):\n",
        "        return ResidualBlock(channel_in, channel_out)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        for stage in self.conv_stages:\n",
        "            x = stage(x)\n",
        "        u = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "        return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWHYn08j5kqb"
      },
      "source": [
        "Repeat the procedure of finding the maximum number of layers such that the network is still trainable, this time with the 'ResNet' model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UJLiaGzJv1j",
        "outputId": "c67393ee-a315-43e5-c149-f2900e5c2991"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ResNet with 102 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 22.50% || Test Error 7.70%\n",
            "Epoch 001/003, Train Error 8.24% || Test Error 6.71%\n",
            "Epoch 002/003, Train Error 6.60% || Test Error 6.13%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 102\n",
        "print(\"\\033[33m\\nTraining ResNet with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = ResNet(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ResNet with 105 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 39.61% || Test Error 12.69%\n",
            "Epoch 001/003, Train Error 86.88% || Test Error 90.20%\n",
            "Epoch 002/003, Train Error 90.13% || Test Error 90.20%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 105\n",
        "print(\"\\033[33m\\nTraining ResNet with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = ResNet(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHhjz1IT7rB4"
      },
      "source": [
        "### Interactions: Batch Norm and Initialization\n",
        "\n",
        "Intuitively, batch norm should make the model more robust to changes in the magnitude of the network's weights: informally, scaling up all the elements of a conv layer's filters by a factor of 10 would not affect the network's output as long as there is a batch norm layer following such convolution, as the normalization would undo the scaling.\n",
        "\n",
        "To check how this intuition translates to practical settings, you should change the original 'CNNtanh' model so that it incorporates batch norm layers (like you have done when modifying 'CNNelu' into 'CNNeluBN').\n",
        "\n",
        "The model below should adopt the naive initialization procedure of sampling from a Gaussian with a deviation of 0.05, not the more sophisticated one that you implemented previously"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8405idig8QGw"
      },
      "outputs": [],
      "source": [
        "class CNNtanhBN_oldinit(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(CNNtanhBN_oldinit, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                layers_list.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1))\n",
        "                layers_list.append(nn.BatchNorm2d(channel_out))\n",
        "                layers_list.append(nn.Tanh())\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc = nn.Linear(channel_in*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                m.weight.data.normal_(0, 0.05)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        for stage in self.conv_stages:\n",
        "            x = stage(x)\n",
        "        u = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "        return u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZsUFsl-0-n"
      },
      "source": [
        "Repeat the procedure of finding the maximum number of layers such that the network is still trainable, this time with CNNeluBN_oldinit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training Tanh CNN + BN + naive init with 48 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 26.44% || Test Error 11.50%\n",
            "Epoch 001/003, Train Error 9.55% || Test Error 6.41%\n",
            "Epoch 002/003, Train Error 4.53% || Test Error 3.53%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 48\n",
        "print(\"\\033[33m\\nTraining Tanh CNN + BN + naive init with {} layers, learning rate {}\\033[0m\".format(k,lr))\n",
        "model = CNNtanhBN_oldinit(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training Tanh CNN + BN + naive init with 51 layers, learning rate 0.05\u001b[0m\n",
            "Epoch 000/003, Train Error 20.38% || Test Error 19.12%\n",
            "Epoch 001/003, Train Error 89.45% || Test Error 88.65%\n",
            "Epoch 002/003, Train Error 89.76% || Test Error 88.65%\n"
          ]
        }
      ],
      "source": [
        "lr = 0.05\n",
        "k = 51\n",
        "print(\"\\033[33m\\nTraining Tanh CNN + BN + naive init with {} layers, learning rate {}\\033[0m\".format(k,lr))\n",
        "model = CNNtanhBN_oldinit(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkrAaQgIBnQt"
      },
      "source": [
        "### Interactions: Batch Norm and Residual Connections\n",
        "\n",
        "Lastly, implement and train a CNN with residual connections but without batch normalization layers -- the goal here is to check how residuals interact with normalization.\n",
        "\n",
        "The model below should be exactly like ResNet, except that it should not have batch norm layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "eK05o-N4B3kG"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock_noBN(nn.Module):\n",
        "    def __init__(self, channel_in, channel_out):\n",
        "        super(ResidualBlock_noBN, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x) + x\n",
        "\n",
        "class ResNet_noBN(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(ResNet_noBN, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                if channel_in == channel_out:\n",
        "                    layers_list.append(self.residual_block(channel_in, channel_out))\n",
        "                else:\n",
        "                    layers_list.append(self.basic_block(channel_in, channel_out))\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc = nn.Linear(channel_in*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1 / (m.kernel_size[0] * m.kernel_size[1] * m.out_channels))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def basic_block(self, channel_in, channel_out):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ELU()\n",
        "        )\n",
        "\n",
        "    def residual_block(self, channel_in, channel_out):\n",
        "        return ResidualBlock_noBN(channel_in, channel_out)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        for stage in self.conv_stages:\n",
        "            x = stage(x)\n",
        "        u = self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "        return u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw991pq-YG1p",
        "outputId": "001fc811-256e-436f-ad30-5c754a00ee88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ResNet w/o BN with 21 layers, learning rate 0.005\u001b[0m\n",
            "Epoch 000/003, Train Error 12.41% || Test Error 2.78%\n",
            "Epoch 001/003, Train Error 2.50% || Test Error 1.87%\n",
            "Epoch 002/003, Train Error 1.88% || Test Error 1.67%\n"
          ]
        }
      ],
      "source": [
        "k = 21\n",
        "lr = 0.005\n",
        "print(\"\\033[33m\\nTraining ResNet w/o BN with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = ResNet_noBN(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ResNet w/o BN with 24 layers, learning rate 0.005\u001b[0m\n",
            "Epoch 000/003, Train Error 90.11% || Test Error 90.20%\n",
            "Epoch 001/003, Train Error 90.13% || Test Error 90.20%\n",
            "Epoch 002/003, Train Error 90.13% || Test Error 90.20%\n"
          ]
        }
      ],
      "source": [
        "k = 24\n",
        "lr = 0.005\n",
        "print(\"\\033[33m\\nTraining ResNet w/o BN with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = ResNet_noBN(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLzjaz0CJFk"
      },
      "source": [
        "Repeat the procedure of finding the maximum number of layers such that the network is still trainable, this time with ResNet_noBN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjZF-f5PCgZ9"
      },
      "source": [
        "### (Optional) Multiple Loss Heads\n",
        "\n",
        "In this optional section, your goal is to incorporate the idea of having multiple loss heads throughout the network, distributed across its depth.\n",
        "\n",
        "For the CNNelu_multihead model below, you should take the CNNelu model that you implemented previously and add two additional classification heads, connected to the outputs of stages 1 and 2.\n",
        "\n",
        "More specifically, the outputs of stages 1 and 2, with shapes 4x14x14 and 8x7x7, should be connected to new fully-connected layers that map them to a 10-dimensional vector (logits for the 10 MNIST classes). The network should output three logit vectors (the original one at the end of the network plus the two new ones) instead of just one, and the loss should be computed as the average of the cross entropies between the true target and each of the three predictions.\n",
        "\n",
        "Note that you will likely have to change the implementation of train_epoch() and test() to accomodate the fact that this model will output three logit vectors instead of one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "Kjq7whPVDR--"
      },
      "outputs": [],
      "source": [
        "class CNNelu_multihead(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(CNNelu_multihead, self).__init__()\n",
        "\n",
        "        # write code here to instantiate layers\n",
        "        # for example, self.conv = nn.Conv2d(1, 4, 3, 1, 1)\n",
        "        # creates a conv layer with 1 input channel, 4 output\n",
        "        # channels, a 3x3 kernel, and stride=padding=1\n",
        "        number_of_stages = 3\n",
        "        layers_per_stage = k // number_of_stages\n",
        "\n",
        "        self.conv_stages = nn.ModuleList()\n",
        "\n",
        "        channel_in = 1\n",
        "        channel_out = 4\n",
        "\n",
        "        for _ in range(number_of_stages):\n",
        "            layers_list = []\n",
        "            for _ in range(layers_per_stage):\n",
        "                layers_list.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, stride=1, padding=1))\n",
        "                layers_list.append(nn.ELU())\n",
        "                channel_in = channel_out\n",
        "            layers_list.append(nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "            channel_out *= 2\n",
        "            self.conv_stages.append(nn.Sequential(*layers_list))\n",
        "\n",
        "        self.fc1 = nn.Linear(4*14*14, 10)\n",
        "        self.fc2 = nn.Linear(8*7*7, 10)\n",
        "        self.fc3 = nn.Linear(16*3*3, 10)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # insert code to compute sigma\n",
        "                sigma = np.sqrt(1 / (m.kernel_size[0] * m.kernel_size[1] * m.out_channels))\n",
        "                m.weight.data.normal_(0, sigma)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input):\n",
        "        # write code here to define how the output u is computed\n",
        "        # from the input and the model's layers\n",
        "        # for example, u = self.conv(input) defines u\n",
        "        # to be simply the output of self.conv given 'input'\n",
        "        x = input\n",
        "        u1, u2, u3 = None, None, None\n",
        "        for idx, stage in enumerate(self.conv_stages):\n",
        "            x = stage(x)\n",
        "            if idx == 0:\n",
        "                u1 = self.fc1(x.view(x.size(0), -1))\n",
        "            elif idx == 1:\n",
        "                u2 = self.fc2(x.view(x.size(0), -1))\n",
        "            else:\n",
        "                u3 = self.fc3(x.view(x.size(0), -1))\n",
        "\n",
        "        return u1, u2, u3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "xX1bRUx9O_S0"
      },
      "outputs": [],
      "source": [
        "def train_multihead(epochs, model, criterion, optimizer, train_loader, test_loader):\n",
        "    for epoch in range(epochs):\n",
        "        train_err = train_epoch_multihead(model, criterion, optimizer, train_loader)\n",
        "        test_err = test_multihead(model, test_loader)\n",
        "        print('Epoch {:03d}/{:03d}, Train Error {:.2f}% || Test Error {:.2f}%'.format(epoch, epochs, train_err*100, test_err*100))\n",
        "    return train_err, test_err\n",
        "\n",
        "def train_epoch_multihead(model, criterion, optimizer, loader):\n",
        "    total_correct = 0.\n",
        "    total_samples = 0.\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "\n",
        "        # insert code to feed the data to the model and collect its output\n",
        "        output = model(data)\n",
        "\n",
        "        # insert code to compute the loss from output and the true target\n",
        "        loss1 = criterion(output[0], target)\n",
        "        loss2 = criterion(output[1], target)\n",
        "        loss3 = criterion(output[2], target)\n",
        "\n",
        "        loss = (loss1 + loss2 + loss3) / 3\n",
        "\n",
        "        # insert code to update total_correct and total_samples\n",
        "        # total_correct: total number of correctly classified samples\n",
        "        # total_samples: total number of samples seen so far\n",
        "        total_correct += ((output[0].argmax(dim=1) == target).sum().item() + \\\n",
        "                            (output[1].argmax(dim=1) == target).sum().item() + \\\n",
        "                            (output[2].argmax(dim=1) == target).sum().item()) / 3\n",
        "        total_samples += target.size(0)\n",
        "\n",
        "        # insert code to update the parameters using optimizer\n",
        "        # be careful in this part as an incorrect implementation will affect\n",
        "        # all your experiments and have a significant impact on your grade!\n",
        "        # in particular, note that pytorch does --not-- automatically\n",
        "        # clear the parameter's gradients: check tutorials to see\n",
        "        # how this can be done with a single method call.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return 1 - total_correct/total_samples\n",
        "\n",
        "def test_multihead(model, loader):\n",
        "    total_correct = 0.\n",
        "    total_samples = 0.\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, target) in enumerate(loader):\n",
        "            if torch.cuda.is_available():\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "\n",
        "            # insert code to feed the data to the model and collect its output\n",
        "            output = model(data)\n",
        "\n",
        "            # insert code to update total_correct and total_samples\n",
        "            # total_correct: total number of correctly classified samples\n",
        "            # total_samples: total number of samples seen so far\n",
        "            total_correct += ((output[0].argmax(dim=1) == target).sum().item() + \\\n",
        "                            (output[1].argmax(dim=1) == target).sum().item() + \\\n",
        "                            (output[2].argmax(dim=1) == target).sum().item()) / 3\n",
        "            total_samples += target.size(0)\n",
        "\n",
        "    return 1 - total_correct/total_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmTspBA6Fnfr"
      },
      "source": [
        "Repeat the procedure of finding the maximum number of layers such that the network is still trainable, this time with CNNelu_multihead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ELU CNN + multiloss with 139 layers, learning rate 0.005\u001b[0m\n",
            "Epoch 000/003, Train Error 81.57% || Test Error 60.38%\n",
            "Epoch 001/003, Train Error 39.57% || Test Error 29.86%\n",
            "Epoch 002/003, Train Error 16.11% || Test Error 7.04%\n"
          ]
        }
      ],
      "source": [
        "l = 0.05\n",
        "k = 139\n",
        "print(\"\\033[33m\\nTraining ELU CNN + multiloss with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNelu_multihead(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train_multihead(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "Training ELU CNN + multiloss with 142 layers, learning rate 0.005\u001b[0m\n",
            "Epoch 000/003, Train Error 88.81% || Test Error 86.13%\n",
            "Epoch 001/003, Train Error 55.47% || Test Error 36.37%\n",
            "Epoch 002/003, Train Error 29.81% || Test Error 21.37%\n"
          ]
        }
      ],
      "source": [
        "l = 0.05\n",
        "k = 142\n",
        "print(\"\\033[33m\\nTraining ELU CNN + multiloss with {} layers, learning rate {}\\033[0m\".format(k, lr))\n",
        "model = CNNelu_multihead(k).cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "train_errs, test_errs = train_multihead(3, model, criterion, optimizer, train_loader, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
